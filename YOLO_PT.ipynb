{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def auto_annotate_videos(video_dir, output_dir):\n",
    "    \"\"\"Generate initial annotations using pre-trained YOLO\"\"\"\n",
    "    model = YOLO('yolo11m-pose.pt')\n",
    "    \n",
    "    for video_file in os.listdir(video_dir):\n",
    "        if not video_file.endswith(('.mp4', '.avi', '.mov')):\n",
    "            continue\n",
    "            \n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            \n",
    "            # Process every 10th frame to reduce dataset size\n",
    "            if frame_count % 10 == 0:\n",
    "                results = model(frame, verbose=False)\n",
    "                \n",
    "                if results[0].keypoints is not None:\n",
    "                    # Save frame\n",
    "                    img_name = f\"{video_file[:-4]}_frame{frame_count:04d}.jpg\"\n",
    "                    img_path = os.path.join(output_dir, 'images', img_name)\n",
    "                    cv2.imwrite(img_path, frame)\n",
    "                    \n",
    "                    # Save annotations\n",
    "                    label_name = f\"{video_file[:-4]}_frame{frame_count:04d}.txt\"\n",
    "                    label_path = os.path.join(output_dir, 'labels', label_name)\n",
    "                    \n",
    "                    with open(label_path, 'w') as f:\n",
    "                        for result in results:\n",
    "                            if result.keypoints is not None:\n",
    "                                # Extract keypoints\n",
    "                                kpts = result.keypoints.xy[0].cpu().numpy()\n",
    "                                conf = result.keypoints.conf[0].cpu().numpy()\n",
    "                                \n",
    "                                # Get bounding box\n",
    "                                box = result.boxes.xywhn[0].cpu().numpy()\n",
    "                                \n",
    "                                # Format: class x_center y_center width height kp1_x kp1_y kp1_v ...\n",
    "                                line = f\"0 {box[0]:.6f} {box[1]:.6f} {box[2]:.6f} {box[3]:.6f}\"\n",
    "                                \n",
    "                                # Normalize keypoints to image dimensions\n",
    "                                h, w = frame.shape[:2]\n",
    "                                for i, (kp, c) in enumerate(zip(kpts, conf)):\n",
    "                                    x_norm = kp[0] / w\n",
    "                                    y_norm = kp[1] / h\n",
    "                                    visibility = 2 if c > 0.5 else 0\n",
    "                                    line += f\" {x_norm:.6f} {y_norm:.6f} {visibility}\"\n",
    "                                \n",
    "                                f.write(line + '\\n')\n",
    "            \n",
    "            frame_count += 1\n",
    "        \n",
    "        cap.release()\n",
    "    \n",
    "    print(f\"Auto-annotation complete. Review and correct labels before training.\")\n",
    "\n",
    "# Usage\n",
    "auto_annotate_videos('raw_videos/', 'pt_exercise_dataset/')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
